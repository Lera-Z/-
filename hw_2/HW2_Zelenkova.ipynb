{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение языка (language detection)\n",
    "--------------------\n",
    "\n",
    "* **Множество случаев** — тексты на разных языках\n",
    "* **Множество классов** — языки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первый метод: частотные слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень простой и неплохой по качеству метод. Сначала создаем частотный словарь для всех языков и берем самые частотные слова. После этого для каждого текста, который нам надо расклассифицировать, смотрим, частотных слов какого языка в нем больше - тот язык и выбираем.\n",
    "\n",
    "Метод неплохо работает на текстах длиннее 50 слов и быстро имлементируется. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве корпусов и текстов для тестирования будем использовать статьи Википедии на разных языках. Скачать Википедию можно различными способами:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Дампы википедии: https://dumps.wikimedia.org/backup-index.html\n",
    "\n",
    "* wikiextractor: http://medialab.di.unipi.it/wiki/Wikipedia_Extractor\n",
    "\n",
    "* annotated_wikiextractor: https://github.com/jodaiber/Annotated-WikiExtractor\n",
    "\n",
    "* wikipedia: https://pypi.python.org/pypi/wikipedia/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Скачаем немного википедии для тестов\n",
    "Воспользуемся пакетом *wikipedia*:\n",
    "\n",
    "`pip install wikipedia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n=10): # функция для скачивания статей из википедии\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print('Skipping page {}'.format(page_name))\n",
    "            continue\n",
    "\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Valeriya/anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /Users/Valeriya/anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Құпия экспедиция\n",
      "Skipping page Борисоглібський монастир\n",
      "Skipping page Сабле (значення)\n",
      "Skipping page Слов'янськ (значення)\n",
      "Skipping page Орлова\n",
      "Skipping page Leisching\n",
      "Skipping page Grąbkowo\n",
      "Skipping page Rudnicki\n",
      "Skipping page Knaap\n",
      "Skipping page Dirtl\n",
      "Skipping page Kartal\n",
      "Skipping page Brugger\n",
      "Skipping page Johann Franz\n",
      "Skipping page Gravesville\n",
      "Skipping page Nowy Majdan\n",
      "Skipping page Baier\n",
      "Skipping page Soldiers' and Sailors' Monument\n",
      "Skipping page Cambria\n",
      "Skipping page Robustel\n",
      "Skipping page Château de Reux (homonymie)\n",
      "Skipping page Hopkinton\n",
      "Skipping page Tenaille (homonymie)\n",
      "Skipping page Priez (homonymie)\n"
     ]
    }
   ],
   "source": [
    "import wikipedia # скачиваем по 100 статей для каждого языка. Это может занять какое-то время (5-10 минут. как правило)\n",
    "\n",
    "wiki_texts = {}\n",
    "for lang in ('kk', 'uk', 'de', 'fr'): # казахский в википедии — это kk,\n",
    "                                      # украинский — uk\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    #print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(wiki_texts['kk'][0]) # распечатаем пару текстов, чтобы убедиться, что все хорошо\n",
    "# print(wiki_texts['de'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Считаем частотный список примерно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "\n",
    "\n",
    "freqs_kk = collections.defaultdict(lambda: 0)\n",
    "corpus_kk = wiki_texts['kk']\n",
    "for article in corpus_kk:\n",
    "    for word in tokenize(article.replace('\\n', '').lower()):\n",
    "        freqs_kk[word] += 1\n",
    "\n",
    "        \n",
    "freqs_uk = collections.defaultdict(lambda: 0)\n",
    "corpus_uk = wiki_texts['uk']\n",
    "for article in corpus_uk:\n",
    "    for word in tokenize(article.replace('\\n', '').lower()):\n",
    "        freqs_uk[word] += 1\n",
    "        \n",
    "\n",
    "        \n",
    "freqs_de = collections.defaultdict(lambda: 0)\n",
    "corpus_de = wiki_texts['de']\n",
    "for article in corpus_de:\n",
    "    for word in tokenize(article.replace('\\n', '').lower()):\n",
    "        freqs_de[word] += 1\n",
    "        \n",
    "        \n",
    "freqs_fr = collections.defaultdict(lambda: 0)\n",
    "corpus_fr = wiki_texts['fr']\n",
    "for article in corpus_fr:\n",
    "    for word in tokenize(article.replace('\\n', '').lower()):\n",
    "        freqs_fr[word] += 1\n",
    "\n",
    "        \n",
    "        \n",
    "# for word in sorted(freqs, key=lambda w: freqs[w], reverse=True)[:50]:\n",
    "#     print('{}\\t{}'.format(freqs[word], word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_dict_kk = set()\n",
    "for key in sorted(freqs_kk, key=lambda w: freqs_kk[w], reverse=True):\n",
    "        if key not in freqs_de and key not in freqs_fr and key not in freqs_uk:\n",
    "            set_dict_kk.add(key)\n",
    "        if len(set_dict_kk) > 499:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_dict_de = set()\n",
    "for key in sorted(freqs_de, key=lambda w: freqs_de[w], reverse=True):\n",
    "        if key not in freqs_kk and key not in freqs_fr and key not in freqs_uk:\n",
    "            set_dict_de.add(key)\n",
    "        if len(set_dict_de) > 499:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_dict_uk = set()\n",
    "for key in sorted(freqs_uk, key=lambda w: freqs_uk[w], reverse=True):\n",
    "        if key not in freqs_kk and key not in freqs_fr and key not in freqs_de:\n",
    "            set_dict_uk.add(key)\n",
    "        if len(set_dict_uk) > 499:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_dict_fr = set()\n",
    "for key in sorted(freqs_fr, key=lambda w: freqs_fr[w], reverse=True):\n",
    "        if key not in freqs_kk and key not in freqs_uk and key not in freqs_de:\n",
    "            set_dict_fr.add(key)\n",
    "        if len(set_dict_fr) > 499:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерируем новые тексты для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Valeriya/anaconda/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /Users/Valeriya/anaconda/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Сарыағаш (айрық)\n",
      "Skipping page Шеол\n",
      "Skipping page Валя-Лунге\n",
      "Skipping page Ель-Міраж\n",
      "Skipping page Крючков\n",
      "Skipping page Подільський провулок\n",
      "Skipping page Walworth\n",
      "Skipping page CNB\n",
      "Skipping page Bodleian\n",
      "Skipping page Rich Creek\n",
      "Skipping page Low-Entry\n",
      "Skipping page John Bradford\n",
      "Skipping page Jim Smith\n",
      "Skipping page Lämmerbichl\n",
      "Skipping page Jakobeit\n",
      "Skipping page Walcha\n",
      "Skipping page Tavera (Begriffsklärung)\n",
      "Skipping page Frappa\n",
      "Skipping page Marthaler\n",
      "Skipping page Michael Geyer\n",
      "Skipping page Schneider-Esleben\n",
      "Skipping page Enki\n",
      "Skipping page Reille\n",
      "Skipping page Inversion\n",
      "Skipping page CPI\n",
      "Skipping page Courtier (homonymie)\n"
     ]
    }
   ],
   "source": [
    "#import wikipedia # скачиваем по 100 статей для каждого языка. Это может занять какое-то время (5-10 минут. как правило)\n",
    "\n",
    "wiki_texts_test = {}\n",
    "for lang in ('kk', 'uk', 'de', 'fr'): # казахский в википедии — это kk,\n",
    "                                      # украинский — uk\n",
    "    wiki_texts_test[lang] = get_texts_for_lang(lang, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_mistakes = 0\n",
    "for i in wiki_texts_test:\n",
    "    for ind in range(len(wiki_texts_test[i])-1):\n",
    "        set_text = set(tokenize(wiki_texts_test[i][ind]))\n",
    "        lengths = []  # kk, de, fr, uk\n",
    "        set_inter_kk = len(set_text & set_dict_kk)\n",
    "        lengths.append(set_inter_kk)\n",
    "        set_inter_de = len(set_text & set_dict_de)\n",
    "        lengths.append(set_inter_de)\n",
    "        set_inter_fr = len(set_text & set_dict_fr)\n",
    "        lengths.append(set_inter_fr)\n",
    "        set_inter_uk = len(set_text & set_dict_uk)\n",
    "        lengths.append(set_inter_uk)\n",
    "        max_num = max(lengths)\n",
    "        if max_num == lengths[0]:\n",
    "            predicted = 'kk'\n",
    "        elif max_num == lengths[1]:\n",
    "            predicted = 'de'\n",
    "        elif max_num == lengths[2]:\n",
    "            predicted = 'fr'\n",
    "        else:\n",
    "            predicted = 'uk'\n",
    "        if predicted != i:\n",
    "            num_of_mistakes += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_all = 0\n",
    "for i in wiki_texts_test:\n",
    "    sum_all += len(wiki_texts_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print((num_of_mistakes)/sum_all) # количество ошибок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нужно сделать это для каждого языка и отфильтровать повторяющиеся"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно загружать готовые частотные словари и классифицировать тексты, просто считая количество слов в каждом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Второй метод: частотные символьные n-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию, которая преобразовывает строку в массив n-грамм заданной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import islice, tee\n",
    "\n",
    "def make_ngrams(text):\n",
    "    N = 3 # задаем длину n-граммы\n",
    "    ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    ngrams = [''.join(x) for x in ngrams]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим частотные словари n-грамм аналогично первому методу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_freqs_fr = collections.defaultdict(lambda: 0)\n",
    "corpus_fr = wiki_texts['fr']\n",
    "for article in corpus_fr:\n",
    "    for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "        n_freqs_fr[ngram] += 1\n",
    "\n",
    "\n",
    "n_freqs_de = collections.defaultdict(lambda: 0)\n",
    "corpus_de = wiki_texts['de']\n",
    "for article in corpus_de:\n",
    "    for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "        n_freqs_de[ngram] += 1       \n",
    "        \n",
    "        \n",
    "n_freqs_uk = collections.defaultdict(lambda: 0)\n",
    "corpus_uk = wiki_texts['uk']\n",
    "for article in corpus_uk:\n",
    "    for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "        n_freqs_uk[ngram] += 1       \n",
    "        \n",
    "        \n",
    "n_freqs_kk = collections.defaultdict(lambda: 0)\n",
    "corpus_kk = wiki_texts['kk']\n",
    "for article in corpus_kk:\n",
    "    for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "        n_freqs_kk[ngram] += 1         \n",
    "        \n",
    "        \n",
    "# for ngram in sorted(freqs, key=lambda n: freqs[n], reverse=True)[:50]:\n",
    "#     print('{}\\t{}'.format(freqs[ngram], ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngr_kk = set()\n",
    "for key in sorted(n_freqs_kk, key=lambda w: n_freqs_kk[w], reverse=True):\n",
    "        if key not in n_freqs_de and key not in n_freqs_fr and key not in n_freqs_uk:\n",
    "            ngr_kk.add(key)\n",
    "        if len(ngr_kk) > 499:\n",
    "            break\n",
    "\n",
    "ngr_de = set()\n",
    "for key in sorted(n_freqs_de, key=lambda w: n_freqs_de[w], reverse=True):\n",
    "        if key not in n_freqs_kk and key not in n_freqs_fr and key not in n_freqs_uk:\n",
    "            ngr_de.add(key)\n",
    "        if len(ngr_de) > 499:\n",
    "            break\n",
    "            \n",
    "ngr_uk = set()\n",
    "for key in sorted(n_freqs_uk, key=lambda w: n_freqs_uk[w], reverse=True):\n",
    "        if key not in n_freqs_kk and key not in n_freqs_fr and key not in n_freqs_de:\n",
    "            ngr_uk.add(key)\n",
    "        if len(ngr_uk) > 499:\n",
    "            break\n",
    "\n",
    "ngr_fr = set()\n",
    "for key in sorted(n_freqs_fr, key=lambda w: n_freqs_fr[w], reverse=True):\n",
    "        if key not in n_freqs_kk and key not in n_freqs_uk and key not in n_freqs_de:\n",
    "            ngr_fr.add(key)\n",
    "        if len(ngr_fr) > 499:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_of_mistakes_ngr = 0\n",
    "for i in wiki_texts_test:\n",
    "    for ind in range(len(wiki_texts_test[i])-1):\n",
    "        set_text = set(make_ngrams(wiki_texts_test[i][ind]))\n",
    "        lengths = []  # kk, de, fr, uk\n",
    "        set_inter_kk = len(set_text & ngr_kk)\n",
    "        lengths.append(set_inter_kk)\n",
    "        set_inter_de = len(set_text & ngr_de)\n",
    "        lengths.append(set_inter_de)\n",
    "        set_inter_fr = len(set_text & ngr_fr)\n",
    "        lengths.append(set_inter_fr)\n",
    "        set_inter_uk = len(set_text & ngr_uk)\n",
    "        lengths.append(set_inter_uk)\n",
    "        max_num = max(lengths)\n",
    "        if max_num == lengths[0]:\n",
    "            predicted = 'kk'\n",
    "        elif max_num == lengths[1]:\n",
    "            predicted = 'de'\n",
    "        elif max_num == lengths[2]:\n",
    "            predicted = 'fr'\n",
    "        else:\n",
    "            predicted = 'uk'\n",
    "        if predicted != i:\n",
    "            num_of_mistakes += 1\n",
    "        #print(i, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print((num_of_mistakes_ngr)/sum_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции для определения языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictionary(text):\n",
    "    set_text = set(tokenize(text))\n",
    "    lengths = []  # kk, de, fr, uk\n",
    "    set_inter_kk = len(set_text & set_dict_kk)\n",
    "    lengths.append(set_inter_kk)\n",
    "    set_inter_de = len(set_text & set_dict_de)\n",
    "    lengths.append(set_inter_de)\n",
    "    set_inter_fr = len(set_text & set_dict_fr)\n",
    "    lengths.append(set_inter_fr)\n",
    "    set_inter_uk = len(set_text & set_dict_uk)\n",
    "    lengths.append(set_inter_uk)\n",
    "    max_num = max(lengths)\n",
    "    if max_num == lengths[0]:\n",
    "        predicted = 'kk'\n",
    "    elif max_num == lengths[1]:\n",
    "        predicted = 'de'\n",
    "    elif max_num == lengths[2]:\n",
    "        predicted = 'fr'\n",
    "    else:\n",
    "        predicted = 'uk'\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngrams(text):\n",
    "    set_text = set(make_ngrams(text))\n",
    "    lengths = []  # kk, de, fr, uk\n",
    "    set_inter_kk = len(set_text & ngr_kk)\n",
    "    lengths.append(set_inter_kk)\n",
    "    set_inter_de = len(set_text & ngr_de)\n",
    "    lengths.append(set_inter_de)\n",
    "    set_inter_fr = len(set_text & ngr_fr)\n",
    "    lengths.append(set_inter_fr)\n",
    "    set_inter_uk = len(set_text & ngr_uk)\n",
    "    lengths.append(set_inter_uk)\n",
    "    max_num = max(lengths)\n",
    "    if max_num == lengths[0]:\n",
    "        predicted = 'kk'\n",
    "    elif max_num == lengths[1]:\n",
    "        predicted = 'de'\n",
    "    elif max_num == lengths[2]:\n",
    "        predicted = 'fr'\n",
    "    else:\n",
    "        predicted = 'uk'\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def what_language(text):\n",
    "    a1 = dictionary(text)\n",
    "    a2 = ngrams(text)\n",
    "    return a1, a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Пес сві́йський або сві́йський соба́ка (Canis lupus familiaris або Canis familiaris) — культигенна тварина. Термін застосовують як для домашніх, так і для бездомних тварин. Свійський пес був одним з найбільш широко застосовуваних службових та компанійських тварин протягом всієї історії людства. За різними оцінками, одомашнення вовка відбулося від 100 000 до 15 000 років тому. mDNA тестування показує, що еволюційний розкол між собаками і вовками відбувся близько 100 000 років тому. Собака швидко став незамінним у всіх світових культурах та був дуже цінним в ранніх людських поселеннях. Зокрема вважають, що успішна еміграція через Берингову протоку могла б бути неможливою без їздових собак. Собаки виконують багато видів робіт для людей, таких як полювання, охорона, служба в поліції та військах, а також собаки допомагають пасти стада худоби, допомагають інвалідам та служать компанійськими сімейними собаками. Ця універсальність, більша ніж практично в будь-якої іншої відомої людству тварини, дала собаці прізвисько «найкращий друг людини». За підрахунками, на планеті на 2015 рік проживало близько 525 мільйонів собак[1]. Завдяки селекції, було розведено сотні різноманітних порід, та зараз виявляють більше поведінкових та морфологічних відмінностей між собаками різних порід, ніж в будь-яких інших наземних ссавців. Наприклад, висота в холці може варіювати від кількох сантиметрів (чивава) до майже метра (ірландський вольфгаунд, великий данець); забарвлення — від білого до чорного, включаючи світло-жовте, сіре, коричневе з великим розмаїттям відтінків.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_2 = 'Der Haushund (Canis lupus familiaris) ist ein Haustier und wird als Heim- und Nutztier gehalten. Seine wilde Stammform ist der Wolf, dem er als Unterart zugeordnet wird. Wann die Domestizierung stattfand ist umstritten; wissenschaftliche Schätzungen variieren zwischen 15.000 und 100.000 Jahren vor unserer Zeit. Im engeren Sinn bezeichnet man als Haushund die Hunde, die überwiegend im Haus gehalten werden, und kennzeichnet damit also eine Haltungsform. Historisch wurde ein Hund, der zur Bewachung des Hauses gehalten wird, als Haushund bezeichnet.[1] Eine weitere Verwendung des Begriffs ist die Einschränkung auf sozialisierte (Haus-)Hunde, also Hunde, die an das Zusammenleben mit Menschen in der menschlichen Gesellschaft gewöhnt und an dieses angepasst sind. Damit wird der Haushund abgegrenzt gegen wild lebende, verwilderte oder streunende Hunde, die zwar auch domestiziert, aber nicht sozialisiert sind.[2] Der Dingo ist ebenfalls ein Haushund, wird jedoch provisorisch als eigenständige Unterart des Wolfes geführt.[3]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('uk', [0, 0, 0, 34]), ('uk', [0, 0, 0, 167]))\n"
     ]
    }
   ],
   "source": [
    "print(what_language(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('de', [0, 25, 0, 0]), ('de', [0, 41, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(what_language(text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
